{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1574b80e-58a4-4298-b8ff-7e6ca928538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: [{'name': 'serving_default_images:0', 'index': 0, 'shape': array([  1,   3, 416, 416]), 'shape_signature': array([  1,   3, 416, 416]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Preprocessed image shape: (1, 3, 416, 416)\n",
      "Preprocessed image dtype: float32\n",
      "Expected input shape: [  1   3 416 416]\n",
      "Preprocessed image shape: (1, 3, 416, 416)\n",
      "Preprocessed image dtype: float32\n",
      "Expected input shape: [  1   3 416 416]\n",
      "Preprocessed image shape: (1, 3, 416, 416)\n",
      "Preprocessed image dtype: float32\n",
      "Expected input shape: [  1   3 416 416]\n",
      "Preprocessed image shape: (1, 3, 416, 416)\n",
      "Preprocessed image dtype: float32\n",
      "Expected input shape: [  1   3 416 416]\n",
      "Preprocessed image shape: (1, 3, 416, 416)\n",
      "Preprocessed image dtype: float32\n",
      "Expected input shape: [  1   3 416 416]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Выполнение инференса\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Получение результатов\u001b[39;00m\n\u001b[0;32m     76\u001b[0m output_data \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\.conda\\envs\\ultralytics_env\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:941\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Загрузка модели TFLite\n",
    "interpreter = tf.lite.Interpreter(model_path=\"models/volov8_model_v4.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Получение информации о входных и выходных тензорах\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Выводим информацию о входной форме модели\n",
    "print(\"Input details:\", input_details)\n",
    "\n",
    "# Функция предобработки изображения\n",
    "def preprocess_image(image, input_shape):\n",
    "    # Убедитесь, что изображение имеет 3 канала (RGB)\n",
    "    if len(image.shape) == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 1:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "    \n",
    "    # Изменяем размер до (width, height)\n",
    "    image = cv2.resize(image, (input_shape[3], input_shape[2]))\n",
    "    image = image.astype(np.float32)\n",
    "    image = image / 255.0  # нормализация\n",
    "    image = np.expand_dims(image, axis=0)  # добавляем измерение batch\n",
    "    image = np.transpose(image, (0, 3, 1, 2))  # меняем порядок размерностей на [batch, channels, height, width]\n",
    "    return image\n",
    "\n",
    "# Функция постобработки результатов (пример для YOLO)\n",
    "def postprocess_output(output_data, img_width, img_height):\n",
    "    boxes, scores, classes = [], [], []\n",
    "    for detection in output_data[0]:\n",
    "        score = detection[4]\n",
    "        if score > 0.5:\n",
    "            box = detection[:4] * np.array([img_width, img_height, img_width, img_height])\n",
    "            boxes.append(box.astype(int))\n",
    "            scores.append(score)\n",
    "            classes.append(int(detection[5]))\n",
    "    return boxes, scores, classes\n",
    "\n",
    "# Инициализация камеры\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    input_shape = input_details[0]['shape']\n",
    "    img_height, img_width, _ = frame.shape\n",
    "\n",
    "    # Предобработка кадра\n",
    "    input_data = preprocess_image(frame, input_shape)\n",
    "    \n",
    "    # Вывод отладочной информации о предобработанном изображении\n",
    "    print(\"Preprocessed image shape:\", input_data.shape)\n",
    "    print(\"Preprocessed image dtype:\", input_data.dtype)\n",
    "    print(\"Expected input shape:\", input_shape)\n",
    "\n",
    "    # Установка данных в тензор\n",
    "    try:\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error setting tensor: {e}\")\n",
    "        break\n",
    "\n",
    "    # Выполнение инференса\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Получение результатов\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    boxes, scores, classes = postprocess_output(output_data, img_width, img_height)\n",
    "\n",
    "    # Отображение результатов на кадре\n",
    "    for box, score, cls in zip(boxes, scores, classes):\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        label = f\"Class {cls}: {score:.2f}\"\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Отображение кадра\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Выход по нажатию клавиши 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c307d-0a57-4959-aef7-ae1a9844f409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
